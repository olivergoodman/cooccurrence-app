{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from newspaper import Article\n",
    "# # get article\n",
    "# url1 = \"http://www.nytimes.com/2016/05/05/technology/moores-law-running-out-of-room-tech-looks-for-a-successor.html?smid=pl-share\"\n",
    "# article1 = Article(url1)\n",
    "# article1.download()\n",
    "# article1.parse()\n",
    "# raw1 = article1.text.lower()\n",
    "# raw1 = raw1.encode(\"utf-8\")\n",
    "# raw1.replace(\".\", \"\")\n",
    "# raw1.replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CooccurrenceMatrix:\n",
    "    \"\"\"\n",
    "    corpus        --  text file, declared when object created\n",
    "    data          --  string\n",
    "    sents         --  list of strings\n",
    "    tokens        --  list of strings\n",
    "    unique_tokens --  list of strings\n",
    "    num_tokens    --  int\n",
    "    fdist         --  fdist object\n",
    "    cooc_array    --  2D list\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus \n",
    "        self.data = \"\"\n",
    "        self.sents = []\n",
    "        self.tokens = []\n",
    "        self.unique_tokens = []\n",
    "        self.num_tokens = 0\n",
    "        self.fdist = None\n",
    "        self.cooc_array = np.zeros((num_tokens, num_tokens))\n",
    "        \n",
    "    def prepareCorpus(corpus):\n",
    "        \"\"\"Given a text file, removes punctiation, returns the list of all tokens, list of unique tokens, \n",
    "        list of sentences, the number of unique tokens, and the frequency distribution\"\"\"\n",
    "        with open(corpus, 'r') as myfile:\n",
    "            self.data = myfile.read().replace('\\n', '')\n",
    "        self.data = self.data.lower()\n",
    "        self.sents = sent_tokenize(self.data)\n",
    "        data = removePunctiation(self.data)\n",
    "        self.tokens = word_tokenize(self.data)\n",
    "        self.unique_tokens = set(self.tokens)\n",
    "        self.num_tokens = len(self.unique_tokens)\n",
    "        self.fdist = FreqDist(self.tokens)\n",
    "        return\n",
    "#         return tokens, sents, unique_tokens, num_tokens, data\n",
    "\n",
    "    def removePunctiation(text):\n",
    "        \"\"\"Given a piece of text, removes all punctiation (replacing periods, semicolons, colons with spaces)\"\"\"\n",
    "        exclude = set(string.punctuation)\n",
    "        text = ''.join(ch for ch in data if ch not in exclude)\n",
    "        text = text.replace(\".\", \" \")\n",
    "        text = text.replace(\";\", \" \")\n",
    "        text = text.replace(\":\", \" \")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        return text\n",
    "\n",
    "    def examineSentence(sent):\n",
    "        \"\"\"Given a sentence (list of words), updates cooccurrence array with number of cooccurrences within sentence\"\"\"\n",
    "        for word1 in sent:\n",
    "            for word2 in sent:\n",
    "                index1 = word_indices[word1]\n",
    "                index2 = word_indices[word2]\n",
    "                #ignore cooccurrences with self\n",
    "                if word1 != word2:\n",
    "                    cooc_array[index1][index2] += 1\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens, sents, unique_tokens, num_tokens, data  = prepareCorpus('obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make cooccurrence array\n",
    "cooc_array = np.zeros((num_tokens, num_tokens))\n",
    "# word indices dict: key->word, value->index in array (0 to m)\n",
    "word_indices = {}\n",
    "counter = 0\n",
    "for t in unique_tokens:\n",
    "    word_indices[t] = counter\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill cooccurrence array\n",
    "for s in sents:\n",
    "    s = removePunctiation(s)\n",
    "    words = word_tokenize(s)\n",
    "    examineSentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 133), ('and', 114), ('of', 82), ('to', 70), ('our', 66)]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.items()[:5] #5 most frequent items in fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print cooc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# make into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
