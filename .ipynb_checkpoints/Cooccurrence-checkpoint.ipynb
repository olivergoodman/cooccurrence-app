{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from newspaper import Article\n",
    "# # get article\n",
    "# url1 = \"http://www.nytimes.com/2016/05/05/technology/moores-law-running-out-of-room-tech-looks-for-a-successor.html?smid=pl-share\"\n",
    "# article1 = Article(url1)\n",
    "# article1.download()\n",
    "# article1.parse()\n",
    "# raw1 = article1.text.lower()\n",
    "# raw1 = raw1.encode(\"utf-8\")\n",
    "# raw1.replace(\".\", \"\")\n",
    "# raw1.replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CooccurrenceMatrix:\n",
    "    \"\"\"\n",
    "    corpus        --  text file, declared when object created\n",
    "    data          --  string\n",
    "    sents         --  list of strings, all the sentences in text file (with punctiation)\n",
    "    tokens        --  list of strings, all tokens in text file (with repetitions)\n",
    "    unique_tokens --  list of strings, all unique tokens\n",
    "    num_tokens    --  int, number of unique tokens in text\n",
    "    fdist         --  fdist object\n",
    "    cooc_matrix   --  2D list, initialized to all 0's in prepareCorpus\n",
    "    word_indices  --  dictionary (key=word, value=index in cooc_matrix)\n",
    "    \"\"\"\n",
    "    def __init__(self, c):\n",
    "        self.corpus = c \n",
    "        self.data = \"\"\n",
    "        self.sents = []\n",
    "        self.tokens = []\n",
    "        self.unique_tokens = []\n",
    "        self.num_tokens = 0\n",
    "        self.fdist = None\n",
    "        self.cooc_matrix = None\n",
    "        self.word_indices = {}\n",
    "        \n",
    "    def prepareCorpus(self):\n",
    "        \"\"\"Given a text file, removes punctuation, returns the list of all tokens, list of unique tokens, \n",
    "        list of sentences, the number of unique tokens, and the frequency distribution. \n",
    "        Also initializies cooc_matrix to all 0's with size = num_tokens\"\"\"\n",
    "        with open(self.corpus, 'r') as myfile:\n",
    "            self.data = myfile.read().replace('\\n', '')\n",
    "        self.data = self.data.lower()\n",
    "        self.sents = sent_tokenize(self.data)\n",
    "        self.data = self.removePunctuation(self.data)\n",
    "        self.tokens = word_tokenize(self.data)\n",
    "        self.unique_tokens = set(self.tokens)\n",
    "        self.num_tokens = len(self.unique_tokens)\n",
    "        self.fdist = FreqDist(self.tokens)\n",
    "        self.cooc_matrix = np.zeros((self.num_tokens, self.num_tokens))\n",
    "        return\n",
    "\n",
    "    def fillMatrix(self):\n",
    "        \"\"\"Fills the cooccurrence matrix\"\"\"\n",
    "        self.findIndices()\n",
    "        for s in self.sents:\n",
    "            s = self.removePunctuation(s)\n",
    "            words = word_tokenize(s)\n",
    "            self.examineSentence(words)\n",
    "        print \"Cooccurrence Matrix:\\n\", self.cooc_matrix \n",
    "        return \n",
    "    \n",
    "    def examineSentence(self, sentence): #problem in here\n",
    "        \"\"\"Given a sentence (list of words), updates cooccurrence matrix with number of cooccurrences within sentence\n",
    "        Ignores cooccurrences of word with itself\"\"\"\n",
    "        for word1 in sentence:\n",
    "            for word2 in sentence:\n",
    "                index1 = self.word_indices[word1]\n",
    "                index2 = self.word_indices[word2]\n",
    "                if word1 != word2:\n",
    "                    self.cooc_matrix[index1][index2] += 1\n",
    "        return\n",
    "    \n",
    "    def findIndices(self):\n",
    "        \"\"\"Sets word_indices, where each key is a unique token and each value is a index in the cooc_matrix\"\"\"\n",
    "        counter = 0\n",
    "        for t in self.unique_tokens:\n",
    "            self.word_indices[t] = counter\n",
    "            counter += 1\n",
    "        return\n",
    "\n",
    "    def removePunctuation(self, text):\n",
    "        \"\"\"Given a piece of text, removes all punctuation (replacing periods, semicolons, colons with spaces)\"\"\"\n",
    "        text = text.replace(\".\", \" \")\n",
    "        text = text.replace(\";\", \" \")\n",
    "        text = text.replace(\":\", \".\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        exclude = set(string.punctuation)\n",
    "        text = ''.join(ch for ch in text if ch not in exclude)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooccurrence Matrix:\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  1. ...,  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on obama.txt\n",
    "obama = CooccurrenceMatrix('obama.txt')\n",
    "obama.prepareCorpus()\n",
    "obama.fillMatrix()\n",
    "\n",
    "# test = \"this is a test piece of text. does the punct remove?\"\n",
    "# print obama.removePunctuation(test)\n",
    "\n",
    "# data\n",
    "# sents = []\n",
    "# tokens = []\n",
    "# unique_tokens = []\n",
    "# num_tokens\n",
    "# fdist\n",
    "# cooc_matrix = np.zeros((num_tokens, num_tokens))\n",
    "# word_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens, sents, unique_tokens, num_tokens, data  = prepareCorpus('obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make cooccurrence array\n",
    "cooc_matrix = np.zeros((num_tokens, num_tokens))\n",
    "# word indices dict: key->word, value->index in matrix (0 to m)\n",
    "word_indices = {}\n",
    "counter = 0\n",
    "for t in unique_tokens:\n",
    "    word_indices[t] = counter\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'removePunctuation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-7b977f458eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fill cooccurrence array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremovePunctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexamineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'removePunctuation' is not defined"
     ]
    }
   ],
   "source": [
    "# fill cooccurrence array\n",
    "for s in sents:\n",
    "    s = removePunctiation(s)\n",
    "    words = word_tokenize(s)\n",
    "    examineSentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 133), ('and', 114), ('of', 82), ('to', 70), ('our', 66)]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.items()[:5] #5 most frequent items in fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print cooc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# make into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
